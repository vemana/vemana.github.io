<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Hardware Memory Models: 10 Minute Mental Model | Vemana's Space.</title> <meta name="author" content="Subrahmanyam "> <meta name="description" content="This is Vemana's space. I write about Topics include Software Engineering, Math, Life and anything that catches my fancy! "> <meta name="keywords" content="software-engineering, math"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vemana.github.io/blog/2023/10mmm-memory-model/"> <link rel="stylesheet" href="/assets/css/posts/10mmm-memory-model.css?1341c8946df8a47b66e8d428a240966f"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Vemana's Space.</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <style type="text/css">span.danger{color:red;font-weight:bold}span.good{color:green;font-weight:bold}.monospace td{font-family:monospace,monospace}div.bootstrap-table.bootstrap4{margin-bottom:20px}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">Hardware Memory Models: 10 Minute Mental Model</h1> <p class="post-meta">December 25, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/software-concurrency"> <i class="fas fa-hashtag fa-sm"></i> software-concurrency</a>     ·   <a href="/blog/category/software-engineering"> <i class="fas fa-tag fa-sm"></i> software-engineering</a>   <a href="/blog/category/10-minute-mental-models"> <i class="fas fa-tag fa-sm"></i> 10-minute-mental-models</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#canonical-example">Canonical Example</a></li> <li class="toc-entry toc-h3"><a href="#failed-proof-of-correctness-of-the-queue">Failed ‘Proof’ of correctness of the Queue</a></li> <li class="toc-entry toc-h3"><a href="#the-problem">The Problem</a></li> <li class="toc-entry toc-h3"> <a href="#understanding-memory-models">Understanding Memory Models</a> <ul> <li class="toc-entry toc-h4"><a href="#fixing-the-queue-implementation">Fixing the Queue implementation</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#various-memory-models">Various Memory Models</a></li> <li class="toc-entry toc-h3"><a href="#test-your-mental-model">Test your Mental Model</a></li> <li class="toc-entry toc-h3"><a href="#resources">Resources</a></li> <li class="toc-entry toc-h3"><a href="#appendix">Appendix</a></li> </ul> </div> <hr> <div id="markdown-content"> <blockquote class="block-tip"> <p><a href="/blog/2023/10mmm">Learn more</a> about the 10 Minute Mental Model series</p> </blockquote> <blockquote class="block-danger"> <p>Concurrency is hard. Memory models are hard. There are probably only a handful of people on Earth who have an acceptably small error rate when dealing with concurrency. I am not one of them. So, the odds are that there are mistakes in this mental model (even though I don’t know of any).</p> </blockquote> <h3 id="canonical-example">Canonical Example</h3> <p>Alex is writing a single producer single consumer <a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/SynchronousQueue.html" rel="external nofollow noopener" target="_blank">Synchronous Queue</a> (i.e. blocking queue with max size 1) with</p> <ul> <li>Two cores in the processor</li> <li>Two threads, one producer and one consumer, running on separate cores</li> <li>Shared memory system (i.e. programs write load/store to memory as opposed to async networks ala distributed systems)</li> </ul> <p>… where the <strong>Producer</strong> and <strong>Consumer</strong> loop like so (slot = EMPTY initially, counter = 0 initially)</p> <table class="monospace"> <thead> <tr> <th>Producer(in a loop)</th> <th>Consumer(in a loop)</th> </tr> </thead> <tbody> <tr> <td>x = ++counter;<br>slot=FULL;<br>while(slot == FULL) {}</td> <td>while(slot == EMPTY) {} <br> print(x); <br> slot = EMPTY;</td> </tr> </tbody> </table> <table> <thead> <tr> <th><strong>Expected</strong></th> <th><strong>Reality</strong></th> </tr> </thead> <tbody> <tr> <td>If the producer produces 1, 2 in order, consumer <strong>always</strong> prints 1, 2 in order.</td> <td> <span class="danger">On some processors, this prints 1, 1 occasionally</span>!</td> </tr> </tbody> </table> <h3 id="failed-proof-of-correctness-of-the-queue">Failed ‘Proof’ of correctness of the Queue</h3> <p>Alex goes one step further and rigorously ‘proves’ that <u>all possible interleavings of instructions from the two threads</u> results in the consumer printing 1, 2 in that order. In fact, the <strong>Expected Interleaving</strong> is quite convincing.</p> <multi-threaded-program title="Expected Interleaving of Events. Time flows top to bottom."> <script type="application/json" id="stuff">
    {
        "threadData": {
            "1": {
                    "name": "Producer",
                    "backgroundColor": "purple"
            },
            "2": {
                    "name": "Consumer",
                    "backgroundColor": "yellow"
            }
        },
        "iList": [
            {
                "thread": "1",
                "instr": "writes x=1"
            },
            {
                "thread": "1",
                "instr": "signals slot and awaits its emptying"
            },
            {
                "thread": "2",
                "instr": "discovers slot is filled"
            },
            {
                "thread": "2",
                "instr": "prints 1",
                "classes": ["green"]
            },
            {
                "thread": "2",
                "instr": "signals slot and awaits its filling"
            },
            {
                "thread": "1",
                "instr": "discovers slot is empty"
            },
            {
                "thread": "1",
                "instr": "writes x=2"
            },
            {
                "thread": "1",
                "instr": "signals slot and awaits its emptying"
            },
            {
                "thread": "2",
                "instr": "discovers slot is filled"
            },
            {
                "thread": "2",
                "instr": "prints 2",
                "classes": ["green"]
            },
            {
                "thread": "2",
                "instr": "... continues ..."
            },
            {
                "thread": "1",
                "instr": "... continues ..."
            }
        ]
    }
    </script> </multi-threaded-program> <h3 id="the-problem">The Problem</h3> <p>You see, Alex considered all possible interleavings <u>under the assumption</u> that instructions in each core <u>execute in order</u>. But that ain’t so! <span class="danger">Some processors swap instructions which results in additional interleavings that also need to be analyzed for correctness.</span></p> <multi-threaded-program title="Why 1, 1 is printed."> <script type="application/json" id="stuff">
    {
        "threadData": {
            "1": {
                    "name": "Producer",
                    "backgroundColor": "purple"
            },
            "2": {
                    "name": "Consumer",
                    "backgroundColor": "yellow"
            }
        },
        "iList": [
            {
                "thread": "1",
                "instr": "writes x=1"
            },
            {
                "thread": "1",
                "instr": "signals slot and awaits its emptying"
            },
            {
                "thread": "2",
                "instr": "discovers slot is filled"
            },
            {
                "thread": "2",
                "instr": "prints 1",
                "classes": ["green"]
            },
            {
                "thread": "2",
                "instr": "signals slot and awaits its filling"
            },
            {
                "thread": "1",
                "instr": "discovers slot is empty"
            },
            {
                "thread": "1",
                "instr": "NEXT TWO INSTRUCTIONS ARE SWAPPED",
                "classes": ["red"]
            },
            {
                "thread": "1",
                "instr": "signals slot and awaits its emptying"
            },
            {
                "thread": "2",
                "instr": "discovers slot is filled"
            },
            {
                "thread": "2",
                "instr": "prints 1 AGAIN",
                "classes": ["red"]
            },
            {
                "thread": "1",
                "instr": "writes x=2"
            },
            {
                "thread": "2",
                "instr": "... continues ..."
            },
            {
                "thread": "1",
                "instr": "... continues ..."
            }
        ]
    }
    </script> </multi-threaded-program> <table> <thead> <tr> <th>FAQ</th> <th>Brief Answer</th> </tr> </thead> <tbody> <tr> <td>Why do processors do these reorderings?</td> <td>For better performance.</td> </tr> <tr> <td>What kind of reorderings are processors allowed to do?</td> <td>Pretty much anything they want. They are just required to document them and support special instructions called Memory Barriers.</td> </tr> <tr> <td>How to write a correct SPSC Synchronous queue?</td> <td>Using Barrier instructions.</td> </tr> <tr> <td>How to make this queue portable across platforms if different processors are different in their memory models?</td> <td>Portable Languages (like Java) have defined their own memory model which can work on a range of processors. To do this, Java uses a different abstraction, called <code class="language-plaintext highlighter-rouge">happens-before</code>. The compiler maps this abstraction to the hardware’s memory barriers. <br><br>When you are coding, you can concentrate on <code class="language-plaintext highlighter-rouge">happens-before</code>. <br><br>When you are trying to squeeze out performance, you will need to work at the memory-barrier level and utilize the particular platform’s specifics for better performance and forgo portability.</td> </tr> <tr> <td>Tell me more about Memory Models &amp; Memory Barriers</td> <td>See below. Memory Model defines what the legal values for <code class="language-plaintext highlighter-rouge">read X</code> are, in a multi-thread access situation. Memory Barriers are programming tools to temporarily restrict the processor to be more orderly so we can reason about our programs.</td> </tr> </tbody> </table> <h3 id="understanding-memory-models">Understanding Memory Models</h3> <p>To reason about Memory Models, we’ll use a reasonable approximation:</p> <ul> <li>Each core executes its own Program</li> <li>Program = Sequence of Instructions</li> <li>Instruction = <code class="language-plaintext highlighter-rouge">Memory Operation | Barrier</code> </li> <li>Memory Operation = <code class="language-plaintext highlighter-rouge">Load (read from memory to register) | Store (write from register to memory)</code> </li> <li>Barrier Instruction = <code class="language-plaintext highlighter-rouge">StoreLoad | LoadStore | LoadLoad | StoreStore</code> </li> <li>Cores can reorder Instructions <ul> <li>Every such re-ordering is a swap of adjacent Instructions in the program</li> <li>A swapped Instruction can be further swapped. For example <code class="language-plaintext highlighter-rouge">A B C D E</code> can be come <code class="language-plaintext highlighter-rouge">E A B C D</code> after multiple swaps of <code class="language-plaintext highlighter-rouge">E</code> </li> <li>Different processors differ on what reorderings they perform</li> </ul> </li> <li>Each of the 4 Barriers prevents a certain subset of adjacent instruction swaps (spot the pattern!) <ul> <li> <code class="language-plaintext highlighter-rouge">Store x=k, StoreLoad</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">Store x=k, StoreStore</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">Load x, LoadStore</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">Load x, LoadLoad</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">StoreLoad, Load x</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">StoreStore, Store x=k</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">LoadLoad, Load x</code> cannot be swapped</li> <li> <code class="language-plaintext highlighter-rouge">LoadStore, Store x=k</code> cannot be swapped</li> <li>The pattern is that a <code class="language-plaintext highlighter-rouge">StoreLoad</code> barrier prevents two kinds of swaps: <ul> <li>Earlier Stores cannot pass the StoreLoad barrier</li> <li>Later Loads cannot pass the StoreLoad barrier</li> <li>Net effect is that a Store before the barrier cannot swap places with a Load that is after the barrier</li> </ul> </li> <li>A more coarse-grained version of these barriers is the <code class="language-plaintext highlighter-rouge">Fence</code> barrier <ul> <li>A <code class="language-plaintext highlighter-rouge">Fence</code> does not participate in any swaps. It is equivalent to all the 4 barriers together.</li> </ul> </li> </ul> </li> </ul> <h4 id="fixing-the-queue-implementation">Fixing the Queue implementation</h4> <p>Alex now writes a queue that works across all the hardware models:</p> <table> <thead> <tr> <th>Producer (in a loop)</th> <th>Consumer (in a loop)</th> </tr> </thead> <tbody> <tr> <td>while(slot == FULL){} <br><span class="good">LoadStore Barrier</span> <br>x = ++counter <br><span class="good">StoreStore Barrier</span> <br>slot = FULL</td> <td>while (slot == EMPTY) {} <br><span class="good">LoadLoad Barrier</span> <br>print(x) <br><span class="good">LoadStore barrier</span> <br> slot = EMPTY</td> </tr> </tbody> </table> <p>Why does this work?</p> <p>First, <strong>notice the parallels to a locking implementation</strong>. Pretend that the check <code class="language-plaintext highlighter-rouge">while(slot == EMPTY){}</code> to be a lock acquire and <code class="language-plaintext highlighter-rouge">slot = EMPTY</code> to be a lock release. Similiary with <code class="language-plaintext highlighter-rouge">while(slot == FULL){}</code> and <code class="language-plaintext highlighter-rouge">slot = FULL</code>. So,</p> <ul> <li>We can be convinced that <code class="language-plaintext highlighter-rouge">x = ++counter</code> is never concurrently executing with <code class="language-plaintext highlighter-rouge">print(x)</code>. That is, what is read for printing will always be something written in a previous slot</li> <li>Similarly, the lock acquisition takes turns. Neither Producer nor Consumer can acquire the lock twice in succession. So, we can be convinced that every value from the Producer will be seen by the Consumer</li> </ul> <p>Second, let’s see how the barriers prevent bad reorderings.</p> <multi-threaded-program title="What goes wrong without the LoadStore barrier in the Producer?"> <script type="application/json" id="stuff">
    {
        "threadData": {
            "1": {
                    "name": "Producer",
                    "backgroundColor": "purple"
            },
            "2": {
                    "name": "Consumer",
                    "backgroundColor": "yellow"
            }
        },
        "iList": [
            {
                "thread": "1",
                "instr": "detects slot is empty"
            },
            {
                "thread": "1",
                "instr": "writes x=1"
            },
            {
                "thread": "1",
                "instr": "signals slot (i.e. set slot = FULL)"
            },
            {
                "thread": "1",
                "instr": "NEXT TWO INSTRUCTIONS ARE SWAPPED that LoadStore barrier would prevent",
                "classes": ["red"]
            },
            {
                "thread": "1",
                "instr": "writes x=2"
            },
            {
                "thread": "1",
                "instr": "loops in while(slot==FULL) {}"
            },
            {
                "thread": "2",
                "instr": "discovers slot is filled"
            },
            {
                "thread": "2",
                "instr": "prints 2. Missed x = 1",
                "classes": ["red"]
            }
        ]
    }
    </script> </multi-threaded-program> <multi-threaded-program title="What goes wrong without the LoadLoad barrier in the Consumer?"> <script type="application/json" id="stuff">
    {
        "threadData": {
            "1": {
                    "name": "Producer",
                    "backgroundColor": "purple"
            },
            "2": {
                    "name": "Consumer",
                    "backgroundColor": "yellow"
            }
        },
        "iList": [
            {
                "thread": "1",
                "instr": "detects slot is empty"
            },
            {
                "thread": "1",
                "instr": "writes x=1"
            },
            {
                "thread": "1",
                "instr": "signals slot (i.e. set slot = FULL)"
            },
            {
                "thread": "2",
                "instr": "detects slot is full"
            },
            {
                "thread": "2",
                "instr": "prints 1"
            },
            {
                "thread": "2",
                "instr": "sets slot = empty"
            },
            {
                "thread": "2",
                "instr": "NEXT TWO INSTRUCTIONS ARE SWAPPED that LoadLoad barrier would prevent",
                "classes": ["red"]
            },
            {
                "thread": "2",
                "instr": "prints 1 again",
                "classes": ["red"]
            },
            {
                "thread": "2",
                "instr": "loops in while(slot==EMPTY) {}"
            },
            {
                "thread": "1",
                "instr": "detects slot is empty"
            }
        ]
    }
    </script> </multi-threaded-program> <p>Similar (bad) things happen without the other barriers.</p> <h3 id="various-memory-models">Various Memory Models</h3> <p>This section lists a bunch of common hardware memory models to provide a taste for them. It doesn’t mention the more recent ARM and RISC V models. It also leaves out a notoriously weak (i.e. very permissive wrt reordering) model used on the DEC Alpha.</p> <table> <tr> <th>Sequential Consistency Model</th> <th>X86 Total Store Ordering (TSO) Model</th> <th>General Relaxed Consistency Model</th> </tr> <tr> <td colspan="3" style="text-align: center;">Instructions <b>appear</b> to execute serially. That is, no two instructions across all the cores <b>appear</b> to execute in parallel.</td> </tr> <tr> <td colspan="3" style="text-align: center;">Instructions from different cores interleave in an unpredictable manner.</td> </tr> <tr> <td>No Instruction swaps are allowed.</td> <td> Only a few specific instruction swaps are allowed. The swaps are not draconian and common patterns like the SPSC synchronous queue shown work without barriers. <br><br>Allowed Single-core swaps. <ol> <li>[Different Address] <code>Store x=k, Load y</code> to <code>Load y, Store x</code> </li> <li>[Same Address] <code>Store x=k, Load x</code> to <code>Load x [but, guaranteed to return k], Store x=k</code>. <br> This is called <b>bypassing</b>. It may be surprising that the <code>Load x</code> returns <code>k</code> even if the actual <code>store x=k</code> has been swapped to AFTER it. Obviously, this is necessary for single-core program correctness. One way to think about this is that the core is intelligent: it knows that the store takes some time to complete. So, in the mean time, it continues with later loads but satisfies them with the correct value for Load (<code>k</code> here). Eventually the Store completes in memory and all other cores can see it; in the mean time, this Core has probably advanced 10s or even 100s of cycles and all the other cores don't see <code>k</code>. </li> </ol> </td> <td> The allowed set of instruction swaps is even larger than TSO. <ol> <li>[Different Address]: All kinds of swaps are allowed. <code> Load x, Load y </code> to <code> Load y, Load x </code>. Similarly, Store/Load, Load/Store, Store/Store swaps are all allowed </li> <li> [Same Address]: This follows TSO rules. Only Store/Load can be swapped with bypassing. That is, <code>Store x=k, Load x</code> to <code>Load x [but, guaranteed to return k], Store x=k</code>. </li> </ol> The set of instructions swaps = Set of instruction swaps allowed by TSO + { Load/Load, Store/Load, Store/Store instruction swaps on different addresses } </td> </tr> <tr> <td>Barriers are essentially no-ops since there are no reorders anyway</td> <td>Only StoreLoad barrier is meaningful. Other barriers are no-ops because their corresponding swaps are not permitted and so the barrier doesn't change the set of permitted re-orderings (of the non-barrier instructions)</td> <td>All the barriers are meaningful and work as described.</td> </tr> <tr> <td colspan="3"> <code>Load x</code> returns <code>k</code> from the latest <code>Store x=k</code> that executed prior to itself. <br>Note that 'prior' is well-defined since instructions execute in serial order. <br>Note that if <code>Load x</code> is swapped with <code>Store x=k</code> (same <code>x</code>), the Load should return <code> k </code>. </td> </tr> </table> <h3 id="test-your-mental-model">Test your Mental Model</h3> <p>Is it required to have a StoreLoad barrier between <code class="language-plaintext highlighter-rouge">slot = FULL</code> and <code class="language-plaintext highlighter-rouge">while(slot == FULL) {}</code> ?</p> <details><summary>Hint</summary> <p>No.</p> <p>There are two possible outcomes in terms of memory completion order:</p> <ul> <li>No swap: slot = FULL and then while(slot == FULL) {}</li> <li>With a swap: (test slot == FULL) completes N times as part of the while loop, slot = FULL completes, rest of the while loop (test slot == FULL) executes. The first block of N tests will all read FULL for the slot even though the actual write (slot=FULL) only completes later. This is guaranteed by almost all memory models to preserve the sanity of software folks.</li> </ul> <p>Since a Store/Load reordering doesn’t change the final result, a StoreLoad barrier isn’t required.</p> </details> <p><br>Writes get stuck in cpu cache before hitting memory. Sometimes, CPU cache needs to be flushed. Right or Wrong?</p> <details><summary>Hint</summary> <p>Wrong.</p> <p>Cpu cache and memory are one unit in all processors with cache coherence (which is most of the processors in practice). If a write hits cpu cache, it is equivalent to hitting memory. There’s no cpu-cache flushing.</p> <p>This is a common misconception on the internet. What is asynchronous is store-buffers that sit <strong>in-front</strong> of the cpu cache. Suffice it to say that it takes some cycles before even initiating a write to cpu-cache (due to the cache coherence protocol, MESI or MOESI for example). A store-buffer holds the write while the cache-coherence protocol gets ready to accept the write and eventually commits it to the cpu-cache. The cpu can continue on doing other work instead of waiting on completing the write. The store buffer also services loads from the writes that it currently holds. Some times, it is necessary to wait until this store buffer drains and become empty and there are instructions for this. It’s still a ‘drain’ because the drain happens naturally as the cpu-cache becomes ready to accept the stores. It’s NOT a ‘flush’ because it cannot be forced - the cpu cache has to be ready to accept the write. There’s no instruction to flush, only an instruction to ‘wait’ until the store buffer drains.</p> </details> <p><br></p> <h3 id="resources">Resources</h3> <ul> <li> <a href="https://pages.cs.wisc.edu/~markhill/papers/primer2020_2nd_edition.pdf" rel="external nofollow noopener" target="_blank">A primer on Memory Consistency and Cache coherence</a> is both a bible and an accessible read</li> <li> <a href="https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html" rel="external nofollow noopener" target="_blank">Java Memory Model</a> is another accessible and important read</li> <li> <a href="https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook-1c.2023.06.11a.pdf" rel="external nofollow noopener" target="_blank">Is Parallel Programming Hard, And, If So, What Can You Do About It?</a> is a practical book from Linux Kernel developer McKenney</li> </ul> <h3 id="appendix">Appendix</h3> <p>Even more reorderings are possible; <span class="danger">Compilers can transform source code too</span> before the hardware reorders it! Analyzing the resulting reorderings and proving correctness of concurrent programs would be extremely tough.</p> <p>Java which is portable across platforms recognized the need for specifying a language-level memory model so that multithreaded programs have well-defined behavior. By necessity, Java had to adopt a sort of least common denominator across the hardware memory models of most (if not all) of the platforms they run on. So, Java’s memory model is most readily understood as a Relaxed Consistency Memory Model, with the following mappings:</p> <ul> <li>volatile field F; Store F=10 is equivalent to: LoadStore StoreStore F=10 StoreLoad StoreStore</li> <li>volatile field F; Load F is equivalent to: LoadStore StoreStore F=10 StoreLoad StoreStore</li> <li>Entering a <code class="language-plaintext highlighter-rouge">synchronized</code> section is like volatile field read</li> <li>Exiting a <code class="language-plaintext highlighter-rouge">synchronized</code> section is like volatile field write</li> </ul> <p>Apart from reordering protection, there are also additional concerns that Java describes:</p> <ul> <li>Atomicity of 64 bit writes (Java only requires 32 bit writes be atomic)</li> <li>Atomicity of 8 bit writes on 32 bit platforms (avoiding what’s called Tearing)</li> <li>Visibility of memory: Although the Java Memory Model doesn’t require this, in practice, volatile writes also come with an immediacy guarantee in addition to barrier effects. That is, as soon as the volatile write operation completes, every other thread can read this write (if it does a volatile read). In contrast, Java doesn’t require a regular memory write be made immediately visible to other threads. In other words, in practice, volatile reads &amp; writes are linearizable, not just sequentially consistent.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/webscale-swe-skillset/">The Web-scale Software Engineer Skillset</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/testing-calculator-homomorphism/">Can software testing be objective? Hello from Homomorphisms.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/abiyz-intro/">How to think about programs &amp; programming? ABIYZ: inspired by age old tools like arithmetic.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/money-from-symmetry/">What is Money and where does it come from? An answer from symmetry.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/junit-parallel-test-runner/">How to run JUnit tests in parallel with Virtual Threads.</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Subrahmanyam . </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js" integrity="sha256-mm3Re3y7xlvh+yCD+l/Zs1d+PU0AEad93MkWvljfm/s=" crossorigin="anonymous"></script> <script>$(document).ready(function(){mermaid.initialize({startOnLoad:!0,theme:"default"}),window.mermaid.init(undefined,document.querySelectorAll(".language-mermaid"))});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="/assets/js/posts/10mmm-memory-model.js?009fa8a3512d120e9aba952335934baf"></script> </body> </html>